{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "from warnings import simplefilter\n",
    "from datetime import date\n",
    "from shutil import disk_usage\n",
    "from math import floor\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "from zipfile import ZipFile\n",
    "from sys import getsizeof\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "test = Path(r'C:\\Users\\wonhyeong\\downloads\\20211020_10-Q_edgar_data_1456802_0001477932-21-007463.txt') #ixbrl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all file in the path to given df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(file: Path) -> str:\n",
    "    docu_end = '</DOCUMENT'\n",
    "    try:\n",
    "        with open(file, 'r+', encoding='utf-8') as f:\n",
    "            # read and rewrite\n",
    "            text = f.read()\n",
    "            found = text.find(docu_end)\n",
    "            if found == -1:\n",
    "                found = text.find(docu_end.lower())\n",
    "            if found == -1:\n",
    "                raise ValueError('No document end tag found')\n",
    "            text = text[:text.find(docu_end)]\n",
    "            f.close()\n",
    "    except:\n",
    "        try:\n",
    "            f.close()\n",
    "        except:\n",
    "            pass\n",
    "        return file, None, False\n",
    "    return file, text, True\n",
    "\n",
    "def make_folder(file: Union[str, Path]) -> Path:\n",
    "    name = '_'.join([str(file.name),'cleaned'])\n",
    "    cleaned = file.parent / name\n",
    "    done= cleaned / 'done'\n",
    "    errored = cleaned / 'errored'\n",
    "    cleaned.mkdir(exist_ok=True)\n",
    "    done.mkdir(exist_ok=True)\n",
    "    errored.mkdir(exist_ok=True)\n",
    "    return done, errored\n",
    "\n",
    "def save_on_success(file: Path, text: str, done: Path) -> None:\n",
    "    print(f'Saving {file.name}')\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "    file.rename(done / file.name)\n",
    "    \n",
    "\n",
    "def save_on_failure(file: Path, errored_dir: Path) -> None:\n",
    "    print(f'Error in {file.name}')\n",
    "    file.rename(errored_dir / file.name)\n",
    "\n",
    "def get_free_space():\n",
    "    \"\"\"return free space of disk as GB\"\"\"\n",
    "    _,_,free = disk_usage('C:\\\\')\n",
    "    result = round(free / 1024**3, 2)\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "def unzip_file(zip_file: Union[str, Path]) -> None:\n",
    "    \"\"\"Unzip a file to the same directory.\"\"\"\n",
    "    with ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(zip_file.parent)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtr = Path(r'C:\\Users\\wonhyeong\\workings\\data\\10X\\QTR4')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root = Path(r'D:\\10-X')\n",
    "    root_dir = [x for x in root.iterdir() if x.is_dir()]\n",
    "    # list of files\n",
    "    sub_dir = [list(f.iterdir()) for f in root_dir if f.is_dir()]\n",
    "    sub_dir = sub_dir[1:]\n",
    "    for quarters in sub_dir:\n",
    "        for qtr in quarters:\n",
    "            files = list(qtr.iterdir())\n",
    "            # zip_list = [x for x in files if x.is_file() and x.suffix == '.zip']\n",
    "            done, errored = make_folder(qtr)\n",
    "            # free_space = get_free_space()\n",
    "            # if free_space > 500:\n",
    "            #     unzip_file(zip_list[0])\n",
    "            #     zip_list = zip_list[1:]\n",
    "            #     zip_list[0].unlink()\n",
    "            with Pool(cpu_count() * 2) as p:\n",
    "                result = p.imap(clean_html, files, chunksize=100) \n",
    "                for file, text, success in result:\n",
    "                    if success:\n",
    "                        save_on_success(file, text, done)\n",
    "                    else:\n",
    "                        save_on_failure(file, errored)\n",
    "            print('Done')\n",
    "\n",
    "## 혹시 안되면 아래 버전으로 실행해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtr = Path(r'C:\\Users\\wonhyeong\\workings\\data\\10X\\QTR4')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root = Path(r'D:\\10-X')\n",
    "    root_dir = [x for x in root.iterdir() if x.is_dir()]\n",
    "    # list of files\n",
    "    sub_dir = [list(f.iterdir()) for f in root_dir if f.is_dir()]\n",
    "    sub_dir = sub_dir[1:]\n",
    "    for quarters in sub_dir:\n",
    "        for qtr in quarters:\n",
    "            files = list(qtr.iterdir())\n",
    "            done, errored = make_folder(qtr)\n",
    "            for file in files:\n",
    "                path, text, success = clean_html(file)\n",
    "                if success:\n",
    "                    save_on_success(path, text, done)\n",
    "                else:\n",
    "                    save_on_failure(path, errored)\n",
    "            print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_of(path: Path, delta: int) -> str:\n",
    "    \"\"\"Return the basename of the path.\"\"\"\n",
    "    name = '_'.join([path.parent.parent.name, path.parent.name, str(delta)]) + '.parquet'\n",
    "    return name\n",
    "\n",
    "def is_over_512MB(df: pd.DataFrame, file: Path) -> bool:\n",
    "    \"\"\"Return True if the size of the dataframe is over 128MB.\"\"\"\n",
    "    mb_512 = 512 * (1024**2) \n",
    "    if ( getsizeof(df) + file.stat().st_size ) < mb_512:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def to_parquet(df: pd.DataFrame, next: Path, path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Save a dataframe as a parquet file.\"\"\"\n",
    "    if is_over_512MB(df, next):\n",
    "        for i in range(1, 1000):\n",
    "            name = path / get_name_of(path, i)\n",
    "            if not name.exists():\n",
    "                break\n",
    "        df.to_parquet(name, compression='snappy', engine='pyarrow')\n",
    "        print(f'Saving {name}')\n",
    "        df = pd.DataFrame()\n",
    "    return df\n",
    "\n",
    "def get_header(txt: str) -> dict:\n",
    "    \"\"\"Return the header of the txt file.\"\"\"\n",
    "    def find_acc(txt: str) -> str:\n",
    "        return txt[txt.find('ACCESSION NUMBER:') + 17:txt.find('CONFORMED SUBMISSION TYPE:')].strip()\n",
    "    def find_cik(txt: str) -> str:\n",
    "        return txt[txt.find('CENTRAL INDEX KEY:') + 18:txt.find('STANDARD INDUSTRIAL CLASSIFICATION:')].strip()\n",
    "    def find_irs(txt: str) -> str:\n",
    "        return txt[txt.find('IRS NUMBER:') + 11:txt.find('STATE OF INCORPORATION:')].strip()\n",
    "    def find_form(txt: str) -> str:\n",
    "        return txt[txt.find('CONFORMED SUBMISSION TYPE:') + 26:txt.find('PUBLIC DOCUMENT COUNT:')].strip()\n",
    "    def find_name(txt: str) -> str:\n",
    "        return txt[txt.find('COMPANY CONFORMED NAME:') + 23:txt.find('CENTRAL INDEX KEY:')].strip()\n",
    "    def find_date(txt: str) -> str:\n",
    "        return txt[txt.find('FILED AS OF DATE:') + 17:txt.find('DATE AS OF CHANGE:')].strip()\n",
    "    def identify_markup(txt: str) -> str:\n",
    "        text = txt.find('<TEXT>') + 6\n",
    "        markup = txt[text: text+20].strip().casefold()\n",
    "        if len(markup) == 0:\n",
    "            raise ValueError('No markup found.')\n",
    "        elif 'HTML'.casefold() in markup:\n",
    "            return 'html'\n",
    "        elif 'XML'.casefold() in markup:\n",
    "            return 'xml'\n",
    "        elif 'XBRL'.casefold() in markup:\n",
    "            return 'xbrl'\n",
    "        else:\n",
    "            return 'text'\n",
    "            \n",
    "    header = txt[txt.find('<SEC-HEADER>') + 12:txt.find('</SEC-HEADER>')]\n",
    "    header_dict = {\n",
    "        'acc': find_acc(header),\n",
    "        'cik': find_cik(header),\n",
    "        'irs': find_irs(header),\n",
    "        'form': find_form(header),\n",
    "        'coname': find_name(header),\n",
    "        'date': find_date(header),\n",
    "        'markup': identify_markup(txt)\n",
    "    }\n",
    "    return header_dict\n",
    "\n",
    "def get_content(txt: str) -> dict:\n",
    "    \"\"\"Return the content of a file.\"\"\"\n",
    "    content = txt[ txt.find('<TEXT>') + 6:txt.find( '</TEXT>' ) ].strip()\n",
    "    return { 'text': content }\n",
    "\n",
    "def get_single_row(file: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"Return a dataframe of a single row.\"\"\"\n",
    "    txt = file.read_text(encoding='utf-8')\n",
    "    header = get_header(txt)\n",
    "    content = get_content(txt)\n",
    "    # merge two dictionaries\n",
    "    df = pd.DataFrame([{**header, **content}], )\n",
    "    return df\n",
    "\n",
    "def append_df(df: pd.DataFrame, file: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"Return a dataframe from a file.\"\"\"\n",
    "    return df.append(get_single_row(file), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "qtr = Path(r'C:\\Users\\wonhyeong\\workings\\data\\10X\\QTR4_cleaned\\done')\n",
    "files = list(qtr.iterdir())\n",
    "li = []\n",
    "for file in files:\n",
    "    if file.name.endswith('.parquet'):\n",
    "        print(f'Loading {file.name}, size is {file.stat().st_size / (1024**2):.2f}MB')\n",
    "        li.append(pd.read_parquet(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, size is 85.13MB\n",
      "Loading, size is 114.14MB\n",
      "Loading, size is 93.40MB\n",
      "Loading, size is 77.04MB\n",
      "Loading, size is 92.70MB\n",
      "Loading, size is 93.79MB\n",
      "Loading, size is 114.79MB\n",
      "Loading, size is 72.87MB\n",
      "Loading, size is 37.37MB\n",
      "Loading, size is 35.88MB\n",
      "Loading, size is 119.68MB\n",
      "Loading, size is 109.04MB\n",
      "Loading, size is 118.37MB\n",
      "Loading, size is 105.49MB\n",
      "Loading, size is 110.55MB\n",
      "Loading, size is 111.26MB\n",
      "Loading, size is 106.65MB\n",
      "Loading, size is 110.62MB\n",
      "Loading, size is 112.55MB\n",
      "Loading, size is 105.23MB\n",
      "Loading, size is 92.29MB\n",
      "Loading, size is 115.86MB\n"
     ]
    }
   ],
   "source": [
    "for i in li:\n",
    "    print(f'Loading, size is {getsizeof(i) / (1024**2):.2f}MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving C:\\Users\\wonhyeong\\workings\\data\\10X\\QTR4_cleaned\\done_cleaned\\done\\QTR4_cleaned_done_cleaned_1.parquet\n",
      "Saving C:\\Users\\wonhyeong\\workings\\data\\10X\\QTR4_cleaned\\done_cleaned\\done\\QTR4_cleaned_done_cleaned_2.parquet\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# sample test\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "qtr = Path(r'C:\\Users\\wonhyeong\\workings\\data\\10X\\QTR4_cleaned\\done')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    files = list(qtr.iterdir())\n",
    "    done, errored = make_folder(qtr)\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        df = to_parquet(df, file, done)\n",
    "        df = append_df(df, file)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(r'C:\\Users\\wonhyeong\\workings\\data\\10X\\QTR4_cleaned\\done_cleaned\\done')\n",
    "files = list(path.iterdir())\n",
    "li = [pd.read_parquet(file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510.42223072052\n",
      "509.6137046813965\n"
     ]
    }
   ],
   "source": [
    "for i in li:\n",
    "    print(getsizeof(i) / (1024**2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5d32f057a3c8bd6a68a4140140c1c01731d179f143636ed2ae590c641a050cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
