{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make simple web crawler\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool # Pool import하기# selenium crawler\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# dir = Path(r\"C:\\Users\\wonhyeong\\workings\\data\\10X\\cleaned\") # on office\n",
    "dir = Path(r\"/Users/jowonhyeong/Desktop/workspace/data\") # on office\n",
    "index_dir = dir / 'index.pkl'\n",
    "index: pd.DataFrame = pd.read_pickle(index_dir)\n",
    "cik_list = index['cik'].unique()\n",
    "cik_list = list(map(str, cik_list))\n",
    "url_cast = 'https://sec.report/CIK/'\n",
    "url_list = list(map(lambda x: ''.join([url_cast, x]), cik_list))\n",
    "ninety = index.query('name == \"-99\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(df):\n",
    "    url_cast = 'https://www.sec.gov/Archives/edgar/data'\n",
    "    url_list = []\n",
    "    for row in df.itertuples():\n",
    "        cik = row.cik\n",
    "        acc = row.acc\n",
    "        mid_acc = acc.replace('-', '')\n",
    "        end_acc = '-'.join([acc, 'index.html'])\n",
    "        url = '/'.join([url_cast, str(cik), mid_acc, end_acc])\n",
    "        url_list.append((acc, url))\n",
    "    return url_list\n",
    "\n",
    "url_list = get_url(ninety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "options.add_argument(\"lang=ko_KR\") # 한국어!\n",
    "driver = webdriver.Chrome(dir / 'chromedriver', options=options)\n",
    "data = {}\n",
    "\n",
    "def get_data(row):\n",
    "    acc, url = row\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    data[acc] = soup\n",
    "    print(acc, 'done')\n",
    "\n",
    "# with Pool(12) as p:\n",
    "#     p.apply(get_data, url_list)\n",
    "\n",
    "for i in url_list:\n",
    "    get_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing data from sec.gov/Archives/edgar/data/.../...-index.html\n",
    "# data of SEC HEADER\n",
    "\n",
    "def _parsing_addr(soup: BeautifulSoup) -> dict:\n",
    "    dic = {}\n",
    "    address = soup.find_all('div', 'mailer')\n",
    "    addr = address[1].find_all('span').__iter__()\n",
    "    dic['STREET 1'] = next(addr).text\n",
    "    city, state, *_ = next(addr).text.strip().split(' ')\n",
    "    zip_num = _[0] if len(_) else None\n",
    "    dic['CITY'] = city\n",
    "    dic['STATE'] = state\n",
    "    dic['ZIP'] = zip_num\n",
    "    try: \n",
    "      phone_No = next(addr).text\n",
    "    except StopIteration:\n",
    "      phone_No = None\n",
    "    dic['BUSINESS PHONE'] = phone_No\n",
    "    return dic\n",
    "\n",
    "def _parsing_filer(filer: BeautifulSoup) -> dict:\n",
    "    ident = filer.find('p', {'class': 'identInfo'})\n",
    "    name = filer.find('span', {'class': 'companyName'}).text\n",
    "    coname = name.split('(')[0].strip()\n",
    "    cik = name.split('(')[1].split(')')[1].split(':')[1].strip()\n",
    "    addr = _parsing_addr(filer)\n",
    "    # find div that inner text contains 'Business Address'\n",
    "    info = map(lambda x: re.sub('[^A-Za-z0-9\\-\\s\\/\\.]+', '', x.text) , ident.contents)\n",
    "    info = [x.strip() for x in info if 0<len(x.strip())<25]\n",
    "    filer_data = {k: v for k, v in zip(info[::2], info[1::2])}\n",
    "    company_data = {}\n",
    "    company_data['COMPANY CONFORMED NAME'] = coname\n",
    "    company_data['CENTRAL INDEX KEY'] = cik\n",
    "    company_data['STANDARD INDUSTRIAL CLASSIFICATION'] = filer_data.get('SIC')\n",
    "    company_data['IRS NUMBER'] = filer_data.get('IRS No.')\n",
    "    company_data['STATE OF INCORPORATION'] = filer_data.get('State of Incorp.')\n",
    "    company_data['FISCAL YEAR END'] = filer_data.get('Fiscal Year End')\n",
    "    filing_values = {}\n",
    "    filing_values['FORM TYPE'] = filer_data.get('Type')\n",
    "    filing_values['SEC ACT'] = filer_data.get('Act')\n",
    "    filing_values['SEC FILE NUMBER'] = filer_data.get('File No.')\n",
    "    filing_values['FILM NUMBER'] = filer_data.get('Film No.')\n",
    "\n",
    "    filing = {}\n",
    "    filing['COMPANY DATA'] = company_data\n",
    "    filing['FILING VALUES'] = filing_values\n",
    "    filing['ADDRESS'] = addr\n",
    "    filing['FORMER COMPANY'] = {'FORMER CONFORMED NAME' : None, 'DATE OF NAME CHANGE': None}\n",
    "    return filing\n",
    "\n",
    "def get_form_data(soup):\n",
    "    dic = {}\n",
    "    div = v.find('div', {'id': 'formDiv'})\n",
    "    # find what that classname is 'formGrouping'\n",
    "    infohead = div.find_all('div', {'class': 'infoHead'})\n",
    "    info = div.find_all('div', {'class': 'info'})\n",
    "    for head, body in zip(infohead, info):\n",
    "        dic[head.text] = body.text\n",
    "    return dic\n",
    "    \n",
    "def get_filer_data(soup: BeautifulSoup) -> list:\n",
    "    filer = v.find_all('div', {'id': 'filerDiv'})\n",
    "    filer_data = [_parsing_filer(x) for x in filer]\n",
    "    return filer_data\n",
    "\n",
    "def df_to_text(df):\n",
    "  df = df[['ACCESSION NUMBER', 'CONFORMED SUBMISSION TYPE', 'Documents', 'Period of Report', 'Filing Date', 'FILER']]\n",
    "  df.columns = ['ACCESSION NUMBER', 'CONFORMED SUBMISSION TYPE', 'PUBLIC DOCUMENT COUNT', 'CONFORMED PERIOD OF REPORT', 'FILED AS OF DATE', 'FILER']\n",
    "  dic = {}\n",
    "  for row in range(len(df)):\n",
    "    text = ''.join([df['ACCESSION NUMBER'][row], '.hdr.sgml:'.ljust(15), df['FILED AS OF DATE'][row], '\\n'])\n",
    "    for i in df.columns:\n",
    "      if i != 'FILER':\n",
    "        col = ''.join([i, ':']).ljust(35)\n",
    "        text += f'{col}{df[i][row]}\\n'\n",
    "      else:\n",
    "        for j in df[i][row]:\n",
    "          text += '\\nFILER:\\n\\n'\n",
    "          for k, v in j.items():\n",
    "            text += f'\\t{k}:\\n'\n",
    "            for kk, vv in v.items():\n",
    "              kk = ''.join([kk, ':']).ljust(35)\n",
    "              text += f'\\t\\t{kk}{vv}\\n'\n",
    "    dic[df['ACCESSION NUMBER'][row]] = text\n",
    "  return dic\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  df = pd.DataFrame()\n",
    "  for k, v in data.items():\n",
    "      v: BeautifulSoup\n",
    "      dic = {'ACCESSION NUMBER': k}\n",
    "      dic.update(get_form_data(v))\n",
    "      dic['FILER'] = get_filer_data(v)\n",
    "      dic['CONFORMED SUBMISSION TYPE'] = dic['FILER'][0]['FILING VALUES']['FORM TYPE']\n",
    "      df = df.append(dic, ignore_index=True)\n",
    "      \n",
    "  text_dict = df_to_text(df)\n",
    "  \n",
    "\n",
    "# class is 'mailer'\n",
    "# find div that class is 'mialer'\n",
    "# find div that inner text contains 'Business Address'\n",
    "# find div that inner text contains 'Mailing Address'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer = df['FILER'][0]\n",
    "filer[0]['COMPANY DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''.join([df['ACCESSION NUMBER'][0], '.hdr.sgml:'.ljust(15), df['FILED AS OF DATE'][0], '\\n'])\n",
    "for i in df.columns:\n",
    "  if i != 'FILER':\n",
    "    col = ''.join([i, ':']).ljust(35)\n",
    "    text += f'{col}{df[i][0]}\\n'\n",
    "  else:\n",
    "    for j in df[i][0]:\n",
    "      text += '\\nFILER:\\n\\n'\n",
    "      for k, v in j.items():\n",
    "        text += f'\\t{k}:\\n'\n",
    "        for kk, vv in v.items():\n",
    "          kk = ''.join([kk, ':']).ljust(35)\n",
    "          text += f'\\t\\t{kk}{vv}\\n'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a:\n",
    "  print('FILER:\\n')\n",
    "  for k, v in i.items():\n",
    "     print('\\t', k.ljust(30), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('CONFORMED SUBMISSION TYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make simple web crawler\n",
    "def get_html(url):\n",
    "    # get html\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_soup(html):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "print(get_soup(get_html('https://www.sec.gov/Archives/edgar/data/933972/000093639296000235/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "TEST_URL = 'https://intoli.com/blog/making-chrome-headless-undetectable/chrome-headless-test.html'\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "options.add_argument(\"lang=ko_KR\") # 한국어!\n",
    "driver = webdriver.Chrome('chromedriver', chrome_options=options)\n",
    "\n",
    "driver.get(TEST_URL)\n",
    "driver.execute_script(\"Object.defineProperty(navigator, 'plugins', {get: function() {return[1, 2, 3, 4, 5]}})\")\n",
    "# lanuages 속성을 업데이트해주기\n",
    "driver.execute_script(\"Object.defineProperty(navigator, 'languages', {get: function() {return ['ko-KR', 'ko']}})\")\n",
    "\n",
    "user_agent = driver.find_element_by_css_selector('#user-agent').text\n",
    "plugins_length = driver.find_element_by_css_selector('#plugins-length').text\n",
    "languages = driver.find_element_by_css_selector('#languages').text\n",
    "\n",
    "print('User-Agent: ', user_agent)\n",
    "print('Plugin length: ', plugins_length)\n",
    "print('languages: ', languages)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "options.add_argument(\"lang=ko_KR\") # 한국어!\n",
    "driver = webdriver.Chrome('chromedriver', options=options)\n",
    "data = {}\n",
    "count: int = 0\n",
    "\n",
    "def get_url_list():\n",
    "  url_cast = 'https://sec.report/CIK/'\n",
    "  url_list = list(map(lambda x: ''.join([url_cast, x]), cik_list))\n",
    "  return url_list\n",
    "\n",
    "def find_some_tables(panels: list, text: str) -> Union[None, list]:\n",
    "  for i in panels:\n",
    "    if i.text.find(text) != -1:\n",
    "      trs = i.find_elements_by_tag_name('tr')\n",
    "      contents = [(x.find_element_by_xpath('td[1]').text, x.find_element_by_xpath('td[2]').text) for x in trs]\n",
    "      return contents\n",
    "  return None\n",
    "\n",
    "def find_and_get(url: str):\n",
    "  print(url)\n",
    "  driver.get(url)\n",
    "  print(count)\n",
    "  panels = driver.find_elements_by_class_name('panel')\n",
    "  details = find_some_tables(panels, 'Company Details')\n",
    "  relations = find_some_tables(panels, 'Related SEC Filings')\n",
    "  callback_func({'details': details, 'relations': relations})\n",
    "\n",
    "def callback_func(result):\n",
    "    data[str(cik_list[count])] = result\n",
    "    count = count + 1\n",
    "    print(count)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # pool = Pool(processes=12)\n",
    "    # pool.map(find_and_get, url_list[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5d32f057a3c8bd6a68a4140140c1c01731d179f143636ed2ae590c641a050cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
