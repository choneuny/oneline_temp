{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from bs4.diagnose import diagnose\n",
    "import cchardet\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from os import cpu_count\n",
    "# Inspiration: https://github.com/honnibal/spacy-ray/pull/\n",
    "# 1/files#diff-7ede881ddc3e8456b320afb958362b2aR12-R45\n",
    "from asyncio import Event\n",
    "from typing import Tuple\n",
    "from time import sleep\n",
    "from typing import Union\n",
    "\n",
    "import ray\n",
    "# For typing purposes\n",
    "from ray.actor import ActorHandle\n",
    "from tqdm import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get SEC_HEADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class preprocessor:\n",
    "    def __init__(self, raw_text: Union[str, bytes]):\n",
    "        self.raw:str = self._decode_text(raw_text)\n",
    "        self.state = {'valid_header': True, 'valid_text': True, 'well_processed': True}\n",
    "        self.what_document_is = 'sec-document'\n",
    "        self.header, self.text = self._sectioning(self.raw)\n",
    "        self.markdown = self._determine_mdtype()\n",
    "        self.meta = self.metadata()\n",
    "        self.text = self.process()\n",
    "\n",
    "    def _decode_text(self, txt: Union[str, bytes]):\n",
    "        if isinstance(txt, str):\n",
    "            return txt\n",
    "        try:\n",
    "            dec = str(txt, encoding='utf-8')\n",
    "            # print('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                dec = str(txt, encoding='cp949')\n",
    "                print('cp949')\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    dec = str(txt, encoding='euc-kr')\n",
    "                    print('euc-kr')\n",
    "                except UnicodeDecodeError:\n",
    "                    dec = str(txt, encoding='utf-8', errors='ignore')\n",
    "                    print('utf-8 with ignore')\n",
    "        return dec\n",
    "\n",
    "    def _sectioning(self, raw_text):\n",
    "        pattern_header = re.compile(r'<sec-header>.*</sec-header>', flags=re.I | re.S)\n",
    "        pattern_text = re.compile(r'<text>.*</text>', flags=re.I | re.S)\n",
    "        try:\n",
    "            header = pattern_header.search(raw_text).group()\n",
    "        except:\n",
    "            # 기록 초기 ims-header 분류\n",
    "            try:\n",
    "                ims_header = re.compile(r'<ims-header>.*</ims-header>', flags=re.I | re.S)\n",
    "                header = ims_header.search(raw_text).group()\n",
    "                self.what_document_is = 'ims-document'\n",
    "                print('something is wrong')\n",
    "            except:\n",
    "                header = None\n",
    "                self.what_document_is = 'unknown'\n",
    "                self.state['valid_header'] = False\n",
    "        try:\n",
    "            text = pattern_text.search(raw_text).group()\n",
    "        except:\n",
    "            text = None\n",
    "            self.state['valid_text'] = False\n",
    "        return header, text\n",
    "\n",
    "    def metadata(self):\n",
    "        meta = {}\n",
    "        keys = ('ACCESSIONNUMBER', 'CONFORMEDSUBMISSIONTYPE', 'FILEDASOFDATE', 'COMPANYCONFORMEDNAME', 'CENTRALINDEXKEY', 'IRSNUMBER')\n",
    "        keys_consice =('acc', 'type', 'date', 'coname', 'cik', 'irs')\n",
    "        space = re.compile('\\s+')\n",
    "        if self.state['valid_header']:\n",
    "            for line in self.header.splitlines():\n",
    "                line = space.sub('', line)\n",
    "                for key in keys:\n",
    "                    if key in line:\n",
    "                        meta[key] = line.split(':')[1]\n",
    "            # 다중 filer 고려 필요\n",
    "            meta = {keys_consice[keys.index(k)]:v for k, v in meta.items()}\n",
    "            meta['markdown'] = self.markdown\n",
    "            meta = {**meta, **self.state, **{'document_type': self.what_document_is}}\n",
    "        return meta\n",
    "\n",
    "    def _determine_mdtype(self):\n",
    "        \"\"\" determine markdown type of self.text \"\"\"\n",
    "        # 1. check if it is html or xbrl\n",
    "        # 2. if not, check if it is custom xml\n",
    "        # 3. if not, consider it as markdown or plain text\n",
    "        text = self.text\n",
    "        if text:\n",
    "            html_pattern = re.compile(r'<html.*<body.*</body.*</html', flags=re.I | re.S)\n",
    "            #regex english upper case\n",
    "            xml_pattern = re.compile(r'<[A-Z]+>', flags=re.I)\n",
    "            if html_pattern.search(text):\n",
    "                return 'html'\n",
    "            if len(xml_pattern.findall(text)) > 4:\n",
    "                return 'xml'\n",
    "            return 'plain'\n",
    "        return 'unknown'\n",
    "\n",
    "    def _process_html(self):\n",
    "        html_pattern = re.compile(r'<html.*<body.*</body.*</html', flags=re.I | re.S)\n",
    "        html = html_pattern.search(self.text).group()\n",
    "        html = BeautifulSoup(html, 'lxml')\n",
    "        # 아무래도 item1보다 더 강건한 구분자가 필요함\n",
    "        # 밑의 코드는 Table of Contents(Index)에서 처음으로 나타나는 Part를 포함한 <a href> 태그를 찾아 그 text를 기반으로 part 1...을 찾음\n",
    "        try:\n",
    "            part = html.find('a', text=re.compile('.*PART\\s*(I|1).*', re.I|re.S))\n",
    "            part = part.text.strip().replace(' ', '\\s*') + '.*'\n",
    "            # print(part)\n",
    "            part = html.find('div', recursive=True, text=re.compile(part, re.I |re.S))\n",
    "            html = part.find_all_next()\n",
    "        except:\n",
    "            # print (traceback.format_exc())\n",
    "            try:\n",
    "                part, *_ = html.select('table a')[0]\n",
    "                part = part.find_parent('table')\n",
    "                # print('table found', part)\n",
    "                html = part.find_all_next()\n",
    "            except:\n",
    "                try:\n",
    "                    part = html.find_all('table')\n",
    "                    part, *_ = (t for t in part if len(t.find_all('tr')) > 5)\n",
    "                    # print('table found', part)\n",
    "                    html = part.find_all_next()\n",
    "                except:\n",
    "                    self.state[\"well_processed\"] = False\n",
    "                    html = html.find_all()\n",
    "                    pass\n",
    "\n",
    "        # def check_and_decompose(tag: BeautifulSoup, attr, check_function: function):\n",
    "        #     for i in tag.find_all():\n",
    "        #         if getattr(i, attr):\n",
    "        #             if i.has_attr(attr):\n",
    "        #                 if check_function(i[attr]):\n",
    "        #                     tag.decompose()\n",
    "        blacklist = ['\\&npsp\\;', '\\s+', '\\_+', 'table\\s*of\\s*contents', '.*text-align:\\s*center.*']\n",
    "        center = re.compile(r'.*TEXT-ALIGN\\s*:\\s*center.*', flags=re.I)\n",
    "        for i in html:\n",
    "            i: BeautifulSoup    \n",
    "            if i.name == 'table':\n",
    "                if len(i.find_all('tr')) > 1:\n",
    "                    i.decompose()\n",
    "        for i in html:\n",
    "            if i.attrs:\n",
    "                if i.has_attr('style'):\n",
    "                    if center.search(i['style']):\n",
    "                        i.decompose()\n",
    "        for i in html:\n",
    "            if i.name == 'div':\n",
    "                if i.has_attr('align'):\n",
    "                    if i['align'] == 'center':\n",
    "                        i.decompose()\n",
    "        for i in html:\n",
    "            if i.text:\n",
    "                if any(re.fullmatch(x, i.text, flags=re.I) for x in blacklist):\n",
    "                    i.decompose()\n",
    "        for i in html:\n",
    "            if not i.text.strip():\n",
    "                i.decompose()\n",
    "\n",
    "        # get all text from html\n",
    "        text = '\\n'.join([i.text for i in html])\n",
    "        # remove all js special characters\n",
    "        text = re.sub('\\&\\w+\\;', '', text)\n",
    "        return text\n",
    "        \n",
    "    def _process_xml(self):\n",
    "        def _get_xml_schema(self, xml: str):\n",
    "            # get all tag names using in the xml\n",
    "            # return a list of tag names\n",
    "            soup = BeautifulSoup(xml, 'xml')\n",
    "            tags = soup.find_all()\n",
    "            tags = [x.name for x in tags]\n",
    "            tags = set(tags)\n",
    "            return tags\n",
    "        soup = BeautifulSoup(self.text, 'xml')\n",
    "        \"\"\"\n",
    "        to return without <TABLE> tags, \n",
    "        need a schema to determine which tag is representing table data, which tag is  closing <TABLE>... etc\n",
    "        mostly global closing tag is </C>, but not always.\n",
    "        \"\"\"\n",
    "        # for i in soup.find_all(name=re.compile('table', re.I)):\n",
    "        #     print(i)\n",
    "        #     i.decompose()\n",
    "        return soup.get_text()\n",
    "\n",
    "            \n",
    "    def _process_plain(self):\n",
    "        # go ocr...\n",
    "        # output may contains some unremoved tables, figures, markdown syntax, etc.\n",
    "        return self.text\n",
    "    \n",
    "    def process(self):\n",
    "        if self.markdown == 'html': \n",
    "            return self._process_html()\n",
    "        if self.markdown == 'xml':\n",
    "            return self._process_xml()\n",
    "        return self._process_plain()\n",
    "\n",
    "    def result(self):\n",
    "        # return a dataframe of metadata plus self.text\n",
    "        result = pd.DataFrame(self.meta, index=[0])\n",
    "        result['text'] = self.text.encode('utf-8')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    my_path = Path.cwd() / 'S&P 500'\n",
    "    parquets = (pd.read_parquet(x, engine='pyarrow') for x in my_path.iterdir() if x.is_file())\n",
    "    for idx, i in enumerate(parquets):\n",
    "        if idx == 1: \n",
    "            break\n",
    "        a = i['txt'].values\n",
    "        for j in a:\n",
    "            file = preprocessor(j)\n",
    "            print(file.meta, f'length is {len(file.text)}' if len(file.text) > 300000 or len(file.text) < 3000 else '')\n",
    "            # print(file.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil, gc\n",
    "\n",
    "def auto_garbage_collect(pct=80.0):\n",
    "    if psutil.virtual_memory().percent >= pct:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    my_path = Path.cwd() / 'S&P 500' / '301-497'\n",
    "    parquets = (pd.read_parquet(x, engine='pyarrow') for x in my_path.iterdir() if x.is_file())\n",
    "    name_list = list(my_path.iterdir())\n",
    "    print(next(parquets), type(next(parquets)))\n",
    "    \n",
    "    try:\n",
    "        ray.init()\n",
    "    except RuntimeError:\n",
    "        ray.shutdown()\n",
    "        ray.init()\n",
    "    num_ticks = 1000\n",
    "    # pb = ProgressBar(num_ticks)\n",
    "    # actor = pb.actor\n",
    "    # You can replace this with any arbitrary Ray task/actor.\n",
    "    for idx, i in enumerate(parquets):\n",
    "        tasks_pre_launch = [preprocessor.remote(x) for x in i['txt'].values]\n",
    "        tasks_pre_launch = [task.result.remote() for task in tasks_pre_launch]\n",
    "        tasks = ray.get(tasks_pre_launch)\n",
    "        pd.concat(tasks).to_parquet(f'{name_list[idx].name.split(\".\")[0]}_cleared.parquet', engine='pyarrow')\n",
    "        auto_garbage_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class ProgressBarActor:\n",
    "    counter: int\n",
    "    delta: int\n",
    "    event: Event\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.counter = 0\n",
    "        self.delta = 0\n",
    "        self.event = Event()\n",
    "\n",
    "    def update(self, num_items_completed: int) -> None:\n",
    "        \"\"\"Updates the ProgressBar with the incremental\n",
    "        number of items that were just completed.\n",
    "        \"\"\"\n",
    "        self.counter += num_items_completed\n",
    "        self.delta += num_items_completed\n",
    "        self.event.set()\n",
    "\n",
    "    async def wait_for_update(self) -> Tuple[int, int]:\n",
    "        \"\"\"Blocking call.\n",
    "        Waits until somebody calls `update`, then returns a tuple of\n",
    "        the number of updates since the last call to\n",
    "        `wait_for_update`, and the total number of completed items.\n",
    "        \"\"\"\n",
    "        await self.event.wait()\n",
    "        self.event.clear()\n",
    "        saved_delta = self.delta\n",
    "        self.delta = 0\n",
    "        return saved_delta, self.counter\n",
    "\n",
    "    def get_counter(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the total number of complete items.\n",
    "        \"\"\"\n",
    "        return self.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back on the local node, once you launch your remote Ray tasks, call\n",
    "# `print_until_done`, which will feed everything back into a `tqdm` counter.\n",
    "\n",
    "\n",
    "class ProgressBar:\n",
    "    progress_actor: ActorHandle\n",
    "    total: int\n",
    "    description: str\n",
    "    pbar: tqdm\n",
    "\n",
    "    def __init__(self, total: int, description: str = \"\"):\n",
    "        # Ray actors don't seem to play nice with mypy, generating\n",
    "        # a spurious warning for the following line,\n",
    "        # which we need to suppress. The code is fine.\n",
    "        self.progress_actor = ProgressBarActor.remote()  # type: ignore\n",
    "        self.total = total\n",
    "        self.description = description\n",
    "\n",
    "    @property\n",
    "    def actor(self) -> ActorHandle:\n",
    "        \"\"\"Returns a reference to the remote `ProgressBarActor`.\n",
    "\n",
    "        When you complete tasks, call `update` on the actor.\n",
    "        \"\"\"\n",
    "        return self.progress_actor\n",
    "\n",
    "    def print_until_done(self) -> None:\n",
    "        \"\"\"Blocking call.\n",
    "\n",
    "        Do this after starting a series of remote Ray tasks, to which you've\n",
    "        passed the actor handle. Each of them calls `update` on the actor.\n",
    "        When the progress meter reaches 100%, this method returns.\n",
    "        \"\"\"\n",
    "        pbar = tqdm(desc=self.description, total=self.total)\n",
    "        while True:\n",
    "            delta, counter = ray.get(self.actor.wait_for_update.remote())\n",
    "            pbar.update(delta)\n",
    "            if counter >= self.total:\n",
    "                pbar.close()\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    my_path = Path.cwd() / 'S&P 500'\n",
    "    parquets = (pd.read_parquet(x, engine='pyarrow') for x in my_path.iterdir() if x.is_file())\n",
    "    print(next(parquets), type(next(parquets)))\n",
    "    \n",
    "    try:\n",
    "        ray.init()\n",
    "    except RuntimeError:\n",
    "        ray.shutdown()\n",
    "        ray.init()\n",
    "    num_ticks = 1000\n",
    "    pb = ProgressBar(num_ticks)\n",
    "    actor = pb.actor\n",
    "    # You can replace this with any arbitrary Ray task/actor.\n",
    "    for idx, i in enumerate(parquets):\n",
    "        actor.update.remote(1)\n",
    "        tasks_pre_launch = [preprocessor.remote(x) for x in i['txt'].values]\n",
    "        tasks_pre_launch = [task.result.remote() for task in tasks_pre_launch]\n",
    "        pb.print_until_done()\n",
    "        tasks = ray.get(tasks_pre_launch)\n",
    "        tasks == list(range(num_ticks))\n",
    "        num_ticks == ray.get(actor.get_counter.remote())\n",
    "        result = pd.concat(tasks)\n",
    "        result.to_parquet(f'result_{idx}.parquet', engine='pyarrow')\n",
    "        if idx > 9:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# get Inner Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = ['\\&npsp\\;', '\\s+', '\\_+', 'table\\s*of\\s*contents', '.*text-align:\\s*center.*']\n",
    "text = BeautifulSoup(tt, 'lxml')\n",
    "center = re.compile(r'.*TEXT-ALIGN\\s*:\\s*center.*', flags=re.I)\n",
    "for i in text.find_all():\n",
    "    if i.name == 'table':\n",
    "        if len(i.find_all('tr')) > 1:\n",
    "            i.decompose()\n",
    "        else:\n",
    "            text = i.text\n",
    "            # replace table as <table>text</table>\n",
    "            # p = <p>text</p>\n",
    "            # i.replace_with(p)\n",
    "\n",
    "            i.replace_with(text)\n",
    "\n",
    "\n",
    "for i in text.find_all():\n",
    "    i: BeautifulSoup\n",
    "    # get style\n",
    "    if i.attrs:\n",
    "        if i.has_attr('style'):\n",
    "            if center.search(i['style']):\n",
    "                i.decompose()\n",
    "\n",
    "# get <div align=\"center\"><font size=\"2\">...</font></div>\n",
    "for i in text.find_all():\n",
    "    if i.name == 'div':\n",
    "        if i.has_attr('align'):\n",
    "            if i['align'] == 'center':\n",
    "                i.decompose()\n",
    "\n",
    "# remove if text is not empty or not in blacklist\n",
    "for i in text.find_all():\n",
    "    if i.text:\n",
    "        if any(re.fullmatch(x, i.text, flags=re.I) for x in blacklist):\n",
    "            i.decompose()\n",
    "\n",
    "for i in text:\n",
    "    if not i.text.strip():\n",
    "        i.decompose()\n",
    "# remove all empty tags\n",
    "\n",
    "# center = [x.text.strip() for x in center]\n",
    "# unique = set(center)\n",
    "# print(len(unique))\n",
    "# for i in sorted(unique):\n",
    "#     print(i)\n",
    "\n",
    "aa = text.get_text()\n",
    "item1 = re.compile(r'item\\s*1', flags=re.I)\n",
    "pos = item1.search(aa).span()\n",
    "aa = aa[pos[0]:]\n",
    "print(aa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "del_elements = [f'.//table', f'//*[@id=\"DSPFPageNumber\"]', f'//*[@id=\"PGBRK\"]', \"//@style='text-align:center'\", f'.//head', \"//div[@style='display:none']\"]\n",
    "stop_words = ['☐','☒']\n",
    "\n",
    "\n",
    "# delete all attributes\n",
    "for i in text.find_all():\n",
    "    i.attrs = {}\n",
    "\n",
    "whitelist = ['html', 'head', 'body', 'p', 'div', 'span', 'a', 'img', 'nav', 'ul', 'li', 'ol']\n",
    "# delete all tags not Regular HTML\n",
    "for i in text.find_all():\n",
    "    if i.name not in whitelist:\n",
    "        i.decompose()\n",
    "\n",
    "# delete all meaningless tags\n",
    "for i in text.find_all():\n",
    "    if not i.text:\n",
    "        i.decompose()\n",
    "    if not i.text.stirp():\n",
    "        i.decompose()\n",
    "\n",
    "# delete attributes\n",
    "for i in text.find_all():\n",
    "    i.attrs = {}\n",
    "\n",
    "# delete tags with text == 'table of contents'\n",
    "for i in text.find_all():\n",
    "    if i.text.lower() == 'Table of Contents'.lower():\n",
    "        i.decompose()\n",
    "        \n",
    "# delete overlapping tags\n",
    "for i in text.find_all():\n",
    "    if i.find_all():\n",
    "        for j in i.find_all():\n",
    "            j.decompose()\n",
    "\n",
    "# find all unique tag name\n",
    "tags = [x.name for x in html.find_all()]\n",
    "tags = set(tags)\n",
    "tags\n",
    "\n",
    "# find regular html tags in name\n",
    "whitelist = ['html', 'head', 'body', 'p', 'div', 'span', 'a', 'b', 'img', 'nav', 'ul', 'li', 'ol']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5d32f057a3c8bd6a68a4140140c1c01731d179f143636ed2ae590c641a050cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
