{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import json\n",
    "import string\n",
    "from datetime import datetime\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "# df가 무거워지면 dask 사용\n",
    "import regex # import re.. slow\n",
    "#!python -m spacy download en_core_web_sm\n",
    "# spacy 모듈에 현재 wasabi 버전에러 관련 문제가 있어 주석처리함\n",
    "# 참조 링크 : https://github.com/explosion/spaCy/issues/11236\n",
    "# 현재 해결방법이 있는듯 하나 spacy를 대체하는 것이 좋을 듯 함\n",
    "# sentencizer 메소드 변경 필요\n",
    "# import spacy # too slow...\n",
    "# from spacy.lang.en import English # updated\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b427e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = filing date, ticker, text, coname,... 등의 column이 있는 데이터프레임\n",
    "# %%time\n",
    "# data = None\n",
    "# data['vacant'] = data['content'].apply(lambda x: 1 if \"NoSuchKey\" in x or \"getElementsByTagName\" in x else 0)\n",
    "# print(Counter(data['vacant']))\n",
    "# data = data[(data['vacant']!=1)].drop(['vacant'], axis = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078151fd",
   "metadata": {},
   "source": [
    "# Preprocess Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmv_rules = [\n",
    "  r'(_){2,}', r'(=){2,}', r'(-){2,}', r'(—){2,}', # 연속된 불용어 제거\n",
    "  r'(Table of Contents ){2,}', # 매 페이지 반복되는 tables of contents 제거\n",
    "  r'(.htm)', r'(.txt)', # 첨부파일 확장자 제거.\n",
    "  r'([0-9]{10,10} )', # 숫자 10자리 제거\n",
    "  r'([0-9]{4,4}-[0-9]{2,2}-[0-9]{2,2}){3,}', # <-- 어떤 텍스트?\n",
    "  r'((INDEX)( [0-9][0-9]){2,})', r'((INDEX)( [0-9][0-9][0-9]){2,})', # index 제거\n",
    "  # f'({ticker_list[i]}:[A-Za-z0-9]+)', # ticker 제거 # 개선요망 #우선 주석처리후 실행시켜봄\n",
    " ]\n",
    "\n",
    "repl_rules = [\n",
    "  (\"I TEM\", \"ITEM\"),\n",
    "  # Bill McDonald 교수가 제공하는 자료 내에서 UNITED STATES SECURITIES... 에서의 UNITED STATES가 종종 누락됨.\n",
    "  # 원본 문서에서 문장 사이에 줄바꿈이 있을 시 누락되는 것으로 추정되며 repl_rule 내에서 수정 요함.\n",
    "  (r\"[\\s]*(UNITED STATES SECURITIES AND EXCHANGE COMMISSION)[\\s]*\", \"UNITED STATES SECURITIES AND EXCHANGE COMMISSION\"),\n",
    "  # (r\"[\\s]*(SECURITIES AND EXCHANGE COMMISSION)[\\s]*\", \"UNITED STATES SECURITIES AND EXCHANGE COMMISSION\") # 이건 불필요한 strip, 오문 발생시킴. raw 확인 후 수정 필요\n",
    "  # 이 부분의 코드 수정 요망\n",
    "  (r\"[\\s]*(Table of Contents UNITED STATES FORM)[\\s]*\", \"UNITED STATES SECURITIES AND EXCHANGE COMMISSION\"),\n",
    "  (r\"[\\s]*(UNITED STATES Table of Contents)[\\s]*\", \"UNITED STATES SECURITIES AND EXCHANGE COMMISSION\"),\n",
    "  # 이 부분의 코드 수정 요망, txt데이터와 html 파싱 데이터의 차이점이 있음.\n",
    "  # 이하는 일반적인 텍스트 클리닝임.\n",
    "  (r'[\\n\\t]', ' '), # 엔터, 탭 제거\n",
    "  (r'[\\s]+', ' '), # 연속된 공백 제거\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55a06f",
   "metadata": {},
   "source": [
    "# Preprocessing, Tokenization, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47414f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  for pattern in rmv_rules:\n",
    "    text = regex.sub(pattern, '', text, flags=regex.IGNORECASE)\n",
    "  for i, (pattern, repl) in enumerate(repl_rules):\n",
    "    text = regex.sub(pattern, repl, text, flags=regex.IGNORECASE) \n",
    "  text = text.strip()\n",
    "  return text\n",
    "\n",
    "# This function will be our all-in-one noise removal function\n",
    "def remove_stopwords(tokens):\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens: \n",
    "        # cleaned 함수\n",
    "        # Eliminating the token if its length is less than 2, if it is a punctuation or if it is a stopword\n",
    "        if token not in string.punctuation and len(token) > 2 and token not in stop_words:\n",
    "            # 영어 외 제거\n",
    "            token = re.sub(r\"[^a-zA-Z]\",'', token)\n",
    "            # more 1 times\n",
    "            if token not in string.punctuation and len(token) > 2 and token not in stop_words:\n",
    "                cleaned_tokens.append(token.lower())\n",
    "                cleaned_tokens = list(filter(None, cleaned_tokens))\n",
    "    return cleaned_tokens\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# The reduce_len parameter will allow a maximum of 3 consecutive repeating characters, while trimming the rest\n",
    "# For example, it will tranform the word: 'Helloooooooooo' to: 'Hellooo'\n",
    "tk = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "# The tokenize function will return a list of tokens\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        # First, we will convert the pos_tag output tags to a tag format that the WordNetLemmatizer can interpret\n",
    "        # In general, if a tag starts with NN, the word is a noun and if it stars with VB, the word is a verb.\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f47ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 코드\n",
    "\n",
    "dir = r'C:\\Users\\wonhyeong\\workings\\data\\10X\\sample.txt'\n",
    "text = open(dir, 'r', encoding='utf-8').read()\n",
    "text = text[text.find('Table of Contents'):]\n",
    "prep = preprocess(text)\n",
    "token = tk.tokenize(prep)\n",
    "token = remove_stopwords(token)\n",
    "lemma = lemmatize_sentence(token)\n",
    "\n",
    "with open(r'C:\\Users\\wonhyeong\\workings\\data\\10X\\sample_prep.txt', 'w', encoding='utf-8') as f:\n",
    "  f.write(prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155dce23",
   "metadata": {},
   "source": [
    "# Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176bdd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer') # updated\n",
    "# This uses the rule-based method, rather than the statistical model to split sentences.\n",
    "# For my use case, using en_core_web_sm worked better but.. too slow...\n",
    "\n",
    "# content에 대한 spacy sentencizer 결과 리스트로 저장\n",
    "data['sentence'] = data['content'].apply(lambda x: [sent.text.strip() for sent in nlp(x).sents])\n",
    "  \n",
    "\n",
    "# 리스트 자료형에 대해 ';'.join, 세미콜론은 텍스트 내에서 자주 언급되므로 대체가 필요해 보임 (한 문장 내에서 사용하는 경우 있음)\n",
    "# data['token'] = data['token'].apply(lambda x: \";\".join(x))\n",
    "# data['lemma'] = data['lemma'].apply(lambda x: \";\".join(x))\n",
    "# data['sentence'] = data['sentence'].apply(lambda x: \";\".join(x))\n",
    "# data['filingDate'] = pd.to_datetime(data['filingDate'], format='%Y-%m-%d')\n",
    "# data.to_csv(\"data.csv\", encoding = \"utf-8\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f63848",
   "metadata": {},
   "source": [
    "## 기존 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9863df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun\n",
    "# %%time 이후 여러 셀 실행 --> lprun 사용, 셀 통합\n",
    "if __name__ == \"__main__\": \n",
    "  data['content'] = data['content'].apply(preprocess)\n",
    "  data['token'] = data['content'].apply(lambda x: tk.tokenize(x))\n",
    "  data['token'] = data['token'].apply(lambda x: remove_stopwords(x))\n",
    "  data['lemma'] = data['token'].apply(lambda x: lemmatize_sentence(x))\n",
    "  data.to_csv(\"data.csv\", encoding = \"utf-8\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp1 = 'UNITED STATES SECURITIES AND EXCHANGE COMMISSION'.lower()\n",
    "# temp2 = 'SECURITIES AND EXCHANGE COMMISSION'.lower()\n",
    "\n",
    "# ticker_list = []\n",
    "# temp_list = list(data['ticker'].drop_duplicates())\n",
    "# for k in temp_list:\n",
    "#     ticker_list.extend(k.split(\";\"))\n",
    "# ticker_list = [k.lower() for k in ticker_list]\n",
    "# ticker_list.extend(['iso4217','us-gaap', 'xbrli', 'utr', 'srt', 'country', 'jaws'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba570bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# regex-based preprocessing\n",
    "# needs to organized as class / function\n",
    "\n",
    "data['content'] = data['content'].apply(lambda x: x.replace(\"I TEM\", \"ITEM\"))\n",
    "data['content'] = data['content'].apply(lambda x: x.replace(x, regex.sub(r'[\\n\\t]', ' ', x).strip()))\n",
    "\n",
    "data['content'] = data['content'].apply(lambda x: regex.sub(r'(_){2,}', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: regex.sub(r'(=){2,}', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: regex.sub(r'(-){2,}', '', x))\n",
    "data['content'] = data['content'].apply(lambda x: regex.sub(r'(—){2,}', '', x))\n",
    "\n",
    "###\n",
    "data['content'] = data['content'].apply(lambda x : x.replace(\"Table of Contents UNITED STATES FORM\", \"UNITED STATES SECURITIES AND EXCHANGE COMMISSION\"))\n",
    "data['content'] = data['content'].apply(lambda x : x.replace(\"UNITED STATES Table of Contents\", \"UNITED STATES SECURITIES AND EXCHANGE COMMISSION\"))\n",
    "# united states securities and exchange commission <<-- .lower() 에 대해 regex.sub 대신 flag=re.IGNORECASE 사용\n",
    "# data['content'] = data['content'].apply(lambda x: x[x.lower().find(temp1):].strip() if temp1 in x.lower() else (x[x.lower().find(temp2):].strip() if temp2 in x.lower() else x))\n",
    "###\n",
    "\n",
    "data['content'] = data['content'].apply(lambda x: regex.sub(r'(Table of Contents ){2,}', '', x))\n",
    "\n",
    "# 시간이 많이 걸리는 듯?\n",
    "# 모든 ticker에 대해 루프를 도는 이유가 뭘까? 추가 설명 필요\n",
    "# ex: data['symbol'] 에 대해 실행할 수 있지 않을까?\n",
    "for i in tqdm(range(0, len(ticker_list))):\n",
    "    data['content'] = data['content'].apply(lambda x: x.replace(x, regex.sub(f'({ticker_list[i]}:[A-Za-z0-9]+)', '', x).strip()))\n",
    "data['content'] = data['content'].apply(lambda x: x.replace(x, regex.sub(r'(.htm)', '', x).strip()))\n",
    "data['content'] = data['content'].apply(lambda x: x.replace(x, regex.sub(r'(.txt)', '', x).strip()))\n",
    "\n",
    "data['content'] = data['content'].apply(lambda x: x.replace(x, regex.sub(r\"\\s+\", \" \", x)))\n",
    "\n",
    "data['content'] = data['content'].apply(lambda x: regex.sub(r'([0-9]{10,10} )', '', x).strip())\n",
    "data['content'] = data['content'].apply(lambda x: regex.sub(r'( [0-9]{4,4}-[0-9]{2,2}-[0-9]{2,2}){3,}', '', x).strip())\n",
    "\n",
    "# 의도를 알 수 없음... \n",
    "temp = \"Pursu a nt t o t he\".lower()\n",
    "data['content'] = data['content'].apply(lambda x: x[:x.lower().find(temp)].strip() if temp in x.lower()[-500:] else x)\n",
    "data['content'] = data['content'].apply(lambda x: regex.sub(r'((INDEX)( [0-9][0-9]){2,})', '', x).strip())\n",
    "data['content'] = data['content'].apply(lambda x: regex.sub(r'((INDEX)( [0-9][0-9][0-9]){2,})', '', x).strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f5d32f057a3c8bd6a68a4140140c1c01731d179f143636ed2ae590c641a050cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
